
## üîç Project Summary

This Retrieval-Augmented Generation (RAG) pipeline enables intelligent question answering over PDF documents. It combines document chunking, semantic search using FAISS, and generative answering using DistilGPT2. The entire workflow is served through a FastAPI web interface, with support for experiment tracking via MLflow.

---

## üß± Components Breakdown

| Component       | Description                                                                 |
| --------------- | --------------------------------------------------------------------------- |
| `rag_engine.py` | Handles PDF loading, text chunking, embedding, FAISS indexing, and querying |
| `main.py`       | FastAPI app that serves a simple question-answer interface                  |
| `form.html`     | Jinja2 template used as the web front-end                                   |
| `vector_score/` | Stores generated FAISS index and chunk metadata                             |
| `sample.pdf`    | The input document used for question answering                              |

---

## ‚öôÔ∏è Pipeline Flow

1. **PDF Chunking**
   ‚Üí Loads and breaks PDF text into meaningful chunks
   ‚Üí Filters out short lines (< 40 characters)

2. **Embedding & FAISS Indexing**
   ‚Üí Uses SentenceTransformer (`distilbert-base-nli-stsb-mean-tokens`)
   ‚Üí Creates vector embeddings and stores them in FAISS
   ‚Üí Chunk text is stored in `metadata.pkl`

3. **User Query ‚Üí Answer Generation**
   ‚Üí Input question is embedded
   ‚Üí Top-k relevant chunks retrieved from FAISS
   ‚Üí `distilgpt2` generates an answer based on context

4. **Web Interface (FastAPI)**
   ‚Üí `/` route shows form
   ‚Üí On submit, displays generated answer in same page

5. **MLflow Logging**
   ‚Üí Logs parameters (PDF path, question)
   ‚Üí Logs metrics (chunk count, retrieved chunks)
   ‚Üí Stores FAISS index & metadata as artifacts

---

## üß™ Sample Query Flow

1. **User Input**: *"What is the eligibility criteria for this scheme?"*
2. **Chunks Retrieved**: 3 most relevant paragraphs
3. **Generated Output**: LLM-generated answer using retrieved context
4. **MLflow**: Logs the query, retrieved chunk count, and generated answer

---

## üöÄ Quick Commands

```bash
# Run FastAPI app
uvicorn app.main:app --reload

# Launch MLflow UI
mlflow ui  # Then open http://127.0.0.1:5000/
```

---

## ‚úÖ Status

| Task                      | Done |
| ------------------------- | ---- |
| PDF Chunking              | ‚úÖ    |
| FAISS Indexing            | ‚úÖ    |
| LLM Answer Generation     | ‚úÖ    |
| FastAPI UI                | ‚úÖ    |
| MLflow Integration        | ‚úÖ    |
| Docker (Optional Support) | ‚ö™    |

---