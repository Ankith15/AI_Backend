# ML Pipeline: End-to-End Machine Learning Workflow

This repository contains a complete end-to-end Machine Learning pipeline built using PySpark, MLflow, Airflow, and Docker. The pipeline processes data, trains a model, evaluates its performance, and logs all relevant metrics and artifacts.

## ğŸ“ Folder Structure

```
ML_LLM_Pipeline/
â”œâ”€â”€ dags/                  # Airflow DAGs for orchestration
â”‚   â””â”€â”€ ml_pipeline.py     # Main DAG definition
â”œâ”€â”€ data/                  # Input dataset files (CSV/Parquet)
â”œâ”€â”€ spark_scripts/         # Spark scripts for preprocessing and training
â”‚   â”œâ”€â”€ preprocessing.py   # Data cleaning and feature engineering
â”‚   â””â”€â”€ train_model.py     # Model training and evaluation
â”œâ”€â”€ mlruns/                # MLflow tracking artifacts (auto-generated)
â”œâ”€â”€ Dockerfile             # Docker setup for Spark + Airflow + MLflow
â”œâ”€â”€ docker-compose.yaml    # Container orchestration
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ README.md              # Project overview and instructions
```

---

## ğŸš€ Features

* ğŸ“Š **Data Preprocessing**: Using PySpark for large-scale data cleaning and transformation.
* ğŸ¤– **Model Training**: Logistic Regression or other models trained via Spark MLlib.
* ğŸ“ˆ **MLflow Integration**: Tracks parameters, metrics, models, and artifacts.
* ğŸ“… **Airflow DAGs**: Orchestrates ETL, training, and evaluation workflows.
* ğŸ³ **Dockerized Setup**: Ensures reproducibility and ease of deployment.

---

## âš™ï¸ Prerequisites

* Python 3.8+
* Docker & Docker Compose
* (Optional) Airflow CLI if running outside Docker

---

## ğŸ”§ Setup Instructions

### 1. Clone the Repository

```bash
git clone <your-repo-url>
cd ML_LLM_Pipeline
```

### 2. Build and Start Docker Containers

```bash
docker-compose up -d --build
```

This will spin up:

* Airflow Webserver (at [http://localhost:8080](http://localhost:8080))
* Airflow Scheduler
* PostgreSQL (for Airflow metadata)
* MLflow Tracking Server (optional, if configured)

---

### 3. Initialize Airflow

```bash
docker-compose run airflow-webserver airflow db init
```

### 4. Create Admin User

```bash
docker-compose run airflow-webserver airflow users create \
  --username admin \
  --firstname Ankith \
  --lastname AI \
  --role Admin \
  --email ankith@example.com \
  --password admin
```

---

### 5. Trigger the ML Pipeline DAG

Visit Airflow at [http://localhost:8080](http://localhost:8080)

* Login with:

  * **Username**: `admin`
  * **Password**: `admin`
* Enable and trigger `ml_pipeline` DAG

---

## ğŸ§  DAG Details

The `ml_pipeline.py` DAG consists of the following tasks:

1. **extract\_data**: Reads and validates input data.
2. **preprocess\_data**: Cleans and transforms the dataset.
3. **train\_model**: Trains a classification model using Spark.
4. **evaluate\_model**: Logs performance metrics to MLflow.

---

## ğŸ“Š MLflow UI

If MLflow is enabled:

```bash
mlflow ui
```

Visit: [http://localhost:5000](http://localhost:5000)

* View experiments
* Compare runs
* Download model artifacts

---

## ğŸ“ Notes

* Data should be placed in the `data/` folder
* Models and metrics are logged automatically via MLflow
* Ensure your DAG schedule and default\_args are correctly configured

---

## âœ… TODO (Optional Enhancements)

* Add hyperparameter tuning
* Add model registry & deployment via MLflow
* Integrate unit testing for Spark scripts

